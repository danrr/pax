Things to try out:
==================
Changes to observation space:
    - Observation space is not the same for all agents - memory-1 vs naive
    - Give one agent the episode count
        - How much does history actually just encode the episode count?

- blackbox policy - with layer in front?

- lower initial reward in iterated games with final set of games having higher games

There is a mismatch between the training meta-agent and inner agent:
    - meta-agent is trained using ES
    - inner agent is trained using SGD
- Meta game aware agents but one with a delay (e.g. one has hidden state reset after one timestep)


Other potential environments:
    - https://github.com/dstl/YAWNING-TITAN
    - MuJoCo?
    - Open AI hide and seek - https://openai.com/research/emergent-tool-use


Slides for Vas:
    - figuring out related work
    - reproducing results
    - finding gaps in the literature
